{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa108161-6352-4d2d-b2c9-cad24879b579",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install --force-reinstall amazon-textract-textractor==1.7.1\n",
    "!pip install inflect\n",
    "!pip install langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784bc0a6-8f56-46c7-8ff7-6f6fe3d47989",
   "metadata": {},
   "source": [
    "Installing the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d14636c-4f02-4b2d-874a-cc44de0c85c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "from textractor import Textractor\n",
    "from textractor.visualizers.entitylist import EntityList\n",
    "from textractor.data.constants import TextractFeatures\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.prompts import PromptTemplate\n",
    "import io\n",
    "import inflect\n",
    "import boto3\n",
    "from botocore.config import Config\n",
    "from botocore.config import Config\n",
    "config = Config(\n",
    "    read_timeout=600, \n",
    "    retries = dict(\n",
    "        max_attempts = 5 \n",
    "    )\n",
    ")\n",
    "from langchain.llms.bedrock import Bedrock\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "bedrock_runtime = boto3.client(service_name='bedrock-runtime',region_name='us-east-1',config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e2fefa-df80-48ec-8cfa-17c1099039f1",
   "metadata": {},
   "source": [
    "Utility functions for text generation and embedding generation using a few Models from Amazon SageMaker Jumpstart and Amazon Bedrock. This models were arbitrarily selected, however, you can modify to use the Models of your choice available on Bedrock, SageMaker JumpStart or HuggingFace.\\\n",
    "**Change the placeholders for sagemaker endpoint names for the various models below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8144b0ae-4209-4a0d-aa71-6fa8097d1653",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_emb_(passage, model):\n",
    "    if \"titan\" in model:\n",
    "        response = bedrock_runtime.invoke_model(body=json.dumps({\"inputText\":passage}),\n",
    "                                    modelId=\"amazon.titan-embed-text-v1\", \n",
    "                                    accept=\"application/json\", \n",
    "                                    contentType=\"application/json\")\n",
    "\n",
    "        response_body = json.loads(response.get('body').read())\n",
    "        embedding=response_body['embedding']\n",
    "    elif \"all-mini-lm\" in model:\n",
    "        payload = {'text_inputs': [passage]}\n",
    "        payload = json.dumps(payload).encode('utf-8')\n",
    "\n",
    "        response = SAGEMAKER.invoke_endpoint(EndpointName=\"ALL MINI LM v6 SAGEMAKER ENDPOINT\", \n",
    "                                                    ContentType='application/json',  \n",
    "                                                    Body=payload)\n",
    "\n",
    "        model_predictions = json.loads(response['Body'].read())\n",
    "        embedding = model_predictions['embedding'][0]\n",
    "    return embedding\n",
    "\n",
    "\n",
    "from langchain.llms.sagemaker_endpoint import LLMContentHandler, SagemakerEndpoint\n",
    "parameters ={\n",
    "        \"max_new_tokens\": 100,       \n",
    "        \"return_full_text\": False,\n",
    "        \"temperature\": 0.1\n",
    "}\n",
    "\n",
    "class ContentHandler(LLMContentHandler):\n",
    "    content_type = \"application/json\"\n",
    "    accepts = \"application/json\"\n",
    "\n",
    "    def transform_input(self, prompt: str, model_kwargs={}) -> bytes:\n",
    "        input_str = json.dumps({\"inputs\": prompt, \"parameters\": model_kwargs})\n",
    "        return input_str.encode(\"utf-8\")\n",
    "\n",
    "    def transform_output(self, output: bytes) -> str:\n",
    "        response_json = json.loads(output.read().decode(\"utf-8\"))\n",
    "        return response_json[0][\"generated_text\"]\n",
    "\n",
    "\n",
    "content_handler = ContentHandler()\n",
    "mixtral_instruct = SagemakerEndpoint(\n",
    "    endpoint_name=\"MIXTRAL SAGEMAKER ENDPOINT\",\n",
    "    region_name=\"us-east-1\",\n",
    "    model_kwargs=parameters,\n",
    "    content_handler=content_handler,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c7d155-4e4f-4fca-860f-bce713ceee42",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Document Extraction\n",
    "We would make use of Amazon 10K report as a sample document. We would upload the file to s3, change the **BUCKET NAME** placeholders to your bucket name in s3.\\\n",
    "We then call the textract `start document analysis` function to process the document. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ac6822-8ebb-411e-ba31-a31e27383604",
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload data to s3\n",
    "!aws s3 cp amzn-20221231.pdf s3://BUCKET-NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11f468c-bd49-488e-834c-7ec037d4a6a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "extractor = Textractor(region_name=\"us-east-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1d6899-82bd-49aa-8d64-ff4ebfdb235b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file=\"s3://BUCKET/amzn-20221231.pdf\"\n",
    "doc_id=file_name = os.path.basename(file)\n",
    "\n",
    "document = extractor.start_document_analysis(\n",
    "    file_source=file,\n",
    "    features=[TextractFeatures.LAYOUT,TextractFeatures.TABLES],\n",
    "    client_request_token=doc_id.split('.')[0],\n",
    "    save_image=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efebf919-47a1-462d-8f21-e7ffe23cf6c3",
   "metadata": {},
   "source": [
    "Using the Textractor linearization function we can enrich the extracted content tags and also hide certain page sections like headers and footer and images that may not be useful for data indexing.\\\n",
    "We decide to tag tables, list and title sections to help in effectively identifying and chunking this sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f086761-25bd-4535-b1b1-10724abceae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textractor.data.text_linearization_config import TextLinearizationConfig\n",
    "\n",
    "config = TextLinearizationConfig(\n",
    "    hide_figure_layout=True,\n",
    "    title_prefix=\"<title> \",\n",
    "     title_suffix=\"</title> \",\n",
    "    hide_header_layout=True,\n",
    "    # section_header_prefix=\"<header>\",\n",
    "    # section_header_suffix=\"</header>\",\n",
    "    # table_linearization_format=\"markdown\", \n",
    "    table_prefix=\"<tables><table>\",\n",
    "    table_suffix=\"</table>\",\n",
    "# table_layout_prefix=\"<table_layout>\",\n",
    "# table_layout_suffix=\"</table_layout>\",\n",
    "    # table_column_separator=\"|\",\n",
    "    # table_tabulate_format = 'github',\n",
    "    list_layout_prefix=\"<list>\",\n",
    "    list_layout_suffix=\"</list>\",\n",
    "    hide_footer_layout=True,\n",
    "    hide_page_num_layout=True,\n",
    "    # list_element_prefix=\"&&&\",\n",
    "    # list_element_suffix=\"%%%%\"\n",
    ")\n",
    "\n",
    "print(document.pages[59].get_text(config=config))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b05fa55-fe4e-462e-9908-b8cae7d731d2",
   "metadata": {},
   "source": [
    "The next two cells iterate through the document pages and replace the tables (in plane text) with their csv formats gotten from the Textract Tables feature.\\\n",
    "This helps to wrangle the table effectively when chunking. We decided to use the \"|\" pipe delimited csv as oppossed to ',' delimited to remove the issue of handling commas within cell contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1806be-76d0-42b1-b064-70a95cc7daf3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def split_list_items_(items):\n",
    "    parts = re.split(\"(<list>|</list>)\", items)  \n",
    "    output = []\n",
    "\n",
    "    inside_list = False\n",
    "    list_item = \"\"\n",
    "\n",
    "    for p in parts:\n",
    "        if p == \"<list>\":\n",
    "            inside_list = True\n",
    "            # output.append(p)\n",
    "            list_item=p\n",
    "        elif p == \"</list>\":\n",
    "            inside_list = False\n",
    "            list_item += p\n",
    "            output.append(list_item)\n",
    "            list_item = \"\" \n",
    "        elif inside_list:\n",
    "            list_item += p.strip()\n",
    "        else:\n",
    "            output.extend(p.split('\\n'))\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a011ade1-bd57-49e3-a531-297e3c34d4f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "We filter through each page with a table and replace that free text table with a csv formatted table \n",
    "demarcated by <table> xml tags\n",
    "\"\"\"\n",
    "csv_seperator=\"|\"\n",
    "document_holder={}\n",
    "table_page={}\n",
    "count=0\n",
    "for ids,page in enumerate(document.pages):\n",
    "\n",
    "    table_count=len([word for word in page.get_text(config=config).split() if \"<tables><table>\" in word]) \n",
    "    assert table_count==len(page.tables) \n",
    "    content=page.get_text(config=config).split(\"<tables>\")\n",
    "    document_holder[ids]=[]    \n",
    "    for idx,item in enumerate(content):\n",
    "        if \"<table>\" in item:     \n",
    "\n",
    "            df0=document.tables[count].to_pandas(use_columns=False).to_csv(header=False, index=None,sep=csv_seperator)\n",
    "            row_count=len([x for x in df0.split(\"\\n\") if x]) \n",
    "            if row_count>1:\n",
    "                if not all(value.strip() == '' for value in df0.split(\"\\n\")[0].split(csv_seperator)):\n",
    "                    row_count=1 \n",
    "            df=pd.read_csv(io.StringIO(df0), sep=csv_seperator, \n",
    "                           header=0 if row_count==1 else 1, keep_default_na=False) \n",
    "            df.rename(columns=lambda x: '' if x.startswith('Unnamed:') else x, inplace=True) \n",
    "            table=df.to_csv(index=None, sep=csv_seperator)\n",
    "            if ids in table_page:\n",
    "                table_page[ids].append(table)\n",
    "            else:\n",
    "                table_page[ids]=[table]\n",
    "            pattern = re.compile(r'<table>(.*?)(</table>)', re.DOTALL) \n",
    "            data=item\n",
    "            table_match = re.search(pattern, data)\n",
    "            table_data = table_match.group(1) if table_match else '' \n",
    "            remaining_content = data[table_match.end():] if table_match else data \n",
    "            content[idx]=f\"<table>{table}</table>\" \n",
    "            count+=1\n",
    "            if \"<list>\" in remaining_content: \n",
    "                output=split_list_items_(remaining_content)\n",
    "                output=[x.strip() for x in output if x.strip()]\n",
    "                document_holder[ids].extend([content[idx]]+output)           \n",
    "            else:\n",
    "                document_holder[ids].extend([content[idx]]+[x.strip() for x in remaining_content.split('\\n') if x.strip()]) \n",
    "        else:   \n",
    "            if \"<list>\" in item and \"<table>\" not in item:   \n",
    "                output=split_list_items_(item)\n",
    "                output=[x.strip() for x in output if x.strip()]\n",
    "                document_holder[ids].extend(output)\n",
    "            else:\n",
    "                document_holder[ids].extend([x.strip() for x in item.split(\"\\n\") if x.strip()])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f279709f-fe7a-4065-94c7-5efeeab8d0d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"\\n\".join(document_holder[59]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29dd9cf4-ee87-4233-bd12-01f50c00b8b7",
   "metadata": {},
   "source": [
    "## Document Chunking\n",
    "This cell iterates through the document per page and chunks the document in the following manner:\n",
    "- It looks for the different xml tags to identify the different page sections. \n",
    "    - If a table xml tag is found, it checks if there is a sentence before that table ( the heueristics employed here is that the sentence before a table is usually the table header) and use it a stable headers.Split table by rows until desired chunk is achieved.\n",
    "    append table column names and table headers to chunk.\n",
    "    - If a list is found, split list by items until desired chunk is achieved. Employ same heuristics as above and append list headers to all list chunk.\n",
    "    - If title tags is found append title to all chunks in that page.\n",
    "- Adds an overlap from previous page to keep the connection of information.\n",
    "- we chunk by number of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919c8154-00b5-466b-b3f6-54f91cde19fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "\n",
    "max_words = 200\n",
    "chunks = {}\n",
    "entire_list_chunk=[]\n",
    "table_header_dict={} \n",
    "list_header_dict={}\n",
    "overlap=50\n",
    "for page, lines in document_holder.items():\n",
    "    page_chunks = []\n",
    "    current_chunk = []\n",
    "    num_words = 0   \n",
    "    start_page=0\n",
    "    TITLE=None\n",
    "    for line in lines:\n",
    "        # if page==59:\n",
    "        if line.strip()!= '':\n",
    "            if \"<title>\" in line:   \n",
    "                TITLE=line.split(\"<title>\")[-1].split(\"</title>\")[0]\n",
    "                line=TITLE\n",
    "                TITLE=TITLE.upper().strip()      \n",
    "                first_page_portion=True\n",
    "            if len(current_chunk)<2 and entire_list_chunk and start_page==0:    \n",
    "                cumulative_word_count = 0\n",
    "                items_within_threshold = 0\n",
    "                for item in reversed(entire_list_chunk[-1]):\n",
    "                    words = item.split()\n",
    "                    if cumulative_word_count + len(words) <= overlap:\n",
    "                        cumulative_word_count += len(words)\n",
    "                        items_within_threshold += 1\n",
    "                        current_chunk.insert(0, item)  \n",
    "                    else:\n",
    "                        break\n",
    "             \n",
    "                num_words=cumulative_word_count\n",
    "                start_page=1\n",
    "\n",
    "            next_num_words = num_words + len(re.findall(r'\\w+', line))  \n",
    "\n",
    "            if  \"<table>\" not in line and \"<list>\" not in line:                \n",
    "\n",
    "                if next_num_words > max_words:\n",
    "                    if TITLE :\n",
    "                        if first_page_portion:\n",
    "                            first_page_portion=False                                            \n",
    "                        else:\n",
    "                            current_chunk.insert(0, TITLE.strip())                       \n",
    "\n",
    "                    page_chunks.append(current_chunk)\n",
    "                    entire_list_chunk.append(current_chunk)\n",
    "                    current_chunk = []\n",
    "                    num_words = 0\n",
    "\n",
    "                current_chunk.append(line)    \n",
    "                num_words += len(re.findall(r'\\w+', line))\n",
    "\n",
    "\n",
    "            \"\"\"\n",
    "            Goal is to segment out table items and chunks intelligently.\n",
    "            We chunk the table by rows and for each chunk of the table we append the table column headers\n",
    "            and table headers if any. This way we preserve the table information across each chunks.\n",
    "            This will help improve semantic search where all the chunks relating to a table would be in the \n",
    "            top k=n response giving the LLM mcomplet information on the table.\n",
    "            \"\"\"\n",
    "\n",
    "\n",
    "            if \"<table>\" in line:                      \n",
    "                line_index=lines.index(line)\n",
    "                if line_index!=0 and \"<table>\" not in lines[line_index-1] and \"<list>\" not in lines[line_index-1]: \n",
    "                    header=lines[line_index-1].replace(\"<title>\",\"\").replace(\"</title>\",\"\")\n",
    "                else:\n",
    "                    header=\"\"            \n",
    "                if page in table_header_dict:\n",
    "                    table_header_dict[page].append(header)\n",
    "                else:\n",
    "                    table_header_dict[page]=[header]\n",
    "                table = line.split(\"<table>\")[-1].split(\"</table>\")[0] # get table from demarcators     \n",
    "                df=pd.read_csv(io.StringIO(table), sep=csv_seperator, keep_default_na=False)\n",
    "                df.rename(columns=lambda x: '' if x.startswith('Unnamed:') else x, inplace=True)\n",
    "\n",
    "                table_chunks = []\n",
    "                curr_chunk = [df.columns.to_list()]  \n",
    "                words=len(re.findall(r'\\w+', str(current_chunk)+\" \"+str(curr_chunk)))     \n",
    "                for row in df.itertuples():\n",
    "                    curr_chunk.append(row)         \n",
    "                    words+=len(re.findall(r'\\w+', str(row)))\n",
    "                    if words > max_words:                              \n",
    "                        table_chunks.append(\"\\n\".join([\"|\".join(str(x) for x in curr_chunk[0])] + [\"|\".join(str(x) for x in r) for r in curr_chunk[1:]]))                   \n",
    "                        words = len(re.findall(r'\\w+', str(curr_chunk[0]))) \n",
    "                        tab_chunk=\"\\n\".join([\"|\".join(str(x) for x in curr_chunk[0])] + [\"|\".join(str(x) for x in r) for r in curr_chunk[1:]]) \n",
    "                    \n",
    "                        if header:   \n",
    "                            if current_chunk and current_chunk[-1].strip().lower()==header.strip().lower():                       \n",
    "                                current_chunk.pop(-1)\n",
    "                       \n",
    "                            if TITLE and TITLE.lower().strip() != header.lower().strip():\n",
    "                                if first_page_portion:\n",
    "                                    first_page_portion=False\n",
    "                                else:\n",
    "                                    current_chunk.insert(0, TITLE.strip())                          \n",
    "                            current_chunk.extend([header.strip()+':' if not header.strip().endswith(':') else header.strip() ]+[tab_chunk])\n",
    "                            page_chunks.append(current_chunk)                            \n",
    "                            entire_list_chunk.append([header]+table_chunks) \n",
    "                        else:\n",
    "                            if TITLE:\n",
    "                                if first_page_portion:\n",
    "                                    first_page_portion=False\n",
    "                                else:\n",
    "                                    current_chunk.insert(0, TITLE.strip())               \n",
    "                            current_chunk.extend([tab_chunk])\n",
    "                            page_chunks.append(current_chunk)\n",
    "                            entire_list_chunk.append(table_chunks)\n",
    "\n",
    "                        num_words=0\n",
    "                        current_chunk=[]\n",
    "                        curr_chunk = [curr_chunk[0]]\n",
    "\n",
    "                if curr_chunk != [df.columns.to_list()] and lines.index(line) == len(lines)-1: \n",
    "                    table_chunks.append(\"\\n\".join([\"|\".join(str(x) for x in curr_chunk[0])] + [\"|\".join(str(x) for x in r) for r in curr_chunk[1:]]))\n",
    "                    tab_chunk=\"\\n\".join([\"|\".join(str(x) for x in curr_chunk[0])] + [\"|\".join(str(x) for x in r) for r in curr_chunk[1:]])\n",
    "                    if header: \n",
    "                        if current_chunk and current_chunk[-1].strip().lower()==header.strip().lower():\n",
    "                            current_chunk.pop(-1)  \n",
    "                        if TITLE and TITLE.lower().strip() != header.lower().strip():\n",
    "                            if first_page_portion:\n",
    "                                first_page_portion=False\n",
    "                            else:\n",
    "                                current_chunk.insert(0, TITLE.strip())          \n",
    "                        current_chunk.extend([header.strip()+':' if not header.strip().endswith(':') else header.strip() ]+[tab_chunk])\n",
    "                        page_chunks.append(current_chunk)\n",
    "                        entire_list_chunk.append([header]+table_chunks)\n",
    "                    else:\n",
    "                        if TITLE:\n",
    "                            if first_page_portion:\n",
    "                                first_page_portion=False\n",
    "                            else:\n",
    "                                current_chunk.insert(0, TITLE.strip())                     \n",
    "                        current_chunk.extend([tab_chunk])\n",
    "                        page_chunks.append(current_chunk)\n",
    "                        entire_list_chunk.append(table_chunks)\n",
    "                    num_words=0\n",
    "                    current_chunk=[]\n",
    "                elif curr_chunk != [df.columns.to_list()] and lines.index(line) != len(lines)-1: \n",
    "                    table_chunks.append(\"\\n\".join([\"|\".join(str(x) for x in curr_chunk[0])] + [\"|\".join(str(x) for x in r) for r in curr_chunk[1:]]))\n",
    "                    tab_chunk=\"\\n\".join([\"|\".join(str(x) for x in curr_chunk[0])] + [\"|\".join(str(x) for x in r) for r in curr_chunk[1:]])\n",
    "                    if header:               \n",
    "                        if current_chunk and current_chunk[-1].strip().lower()==header.strip().lower():\n",
    "                            current_chunk.pop(-1) \n",
    "                        current_chunk.extend([header.strip()+':' if not header.strip().endswith(':') else header.strip() ]+[tab_chunk])\n",
    "                    else:\n",
    "                        current_chunk.extend([tab_chunk])                  \n",
    "                    num_words=words\n",
    "\n",
    "            \"\"\"\n",
    "            Goal is to segment out list items and chunk intelligently.\n",
    "            We chunk each list by items in the list and \n",
    "            for each list chunk we append the list header to the chunk to preserve the information of the list across chunks.\n",
    "            This would boost retrieval process where question pertaining to a list will have all list chunks within\n",
    "            the topK=n responses.\n",
    "            \"\"\"\n",
    "\n",
    "            if \"<list>\" in line:\n",
    "                # Get list header which is usually line before list in document\n",
    "                line_index=lines.index(line)\n",
    "                if line_index!=0 and \"<table>\" not in lines[line_index-1] and \"<list>\" not in lines[line_index-1]:\n",
    "                    header=lines[line_index-1].replace(\"<title>\",\"\").replace(\"</title>\",\"\")\n",
    "                else:\n",
    "                    header=\"\"           \n",
    "                list_pattern = re.compile(r'<list>(.*?)</list>', re.DOTALL)     \n",
    "                list_match = re.search(list_pattern, line)\n",
    "                list_ = list_match.group(1)\n",
    "                list_lines=list_.split(\"\\n\")                \n",
    "\n",
    "                curr_chunk = []  \n",
    "                words=len(re.findall(r'\\w+', str(current_chunk)))                \n",
    "                for item in list_lines:\n",
    "                    curr_chunk.append(item)         \n",
    "                    words+=len(re.findall(r'\\w+', item)) \n",
    "\n",
    "                    if words >= max_words: #  \n",
    "                        words = 0\n",
    "\n",
    "                        list_chunk=\"\\n\".join(curr_chunk)\n",
    "                        if header:        \n",
    "                            if current_chunk and current_chunk[-1].strip().lower()==header.strip().lower():                    \n",
    "                                current_chunk.pop(-1)                        \n",
    "                            if TITLE and TITLE.lower().strip() != header.lower().strip():\n",
    "                                if first_page_portion:\n",
    "                                    first_page_portion=False\n",
    "                                else:\n",
    "                                    current_chunk.insert(0, TITLE.strip())                            \n",
    "                            current_chunk.extend([header.strip()+':' if not header.strip().endswith(':') else header.strip() ]+[list_chunk]) \n",
    "                            page_chunks.append(current_chunk)                            \n",
    "                            entire_list_chunk.append([header]+[list_chunk])\n",
    "                        else:\n",
    "                            if TITLE:\n",
    "                                if first_page_portion:\n",
    "                                    first_page_portion=False\n",
    "                                else:\n",
    "                                    current_chunk.insert(0, TITLE.strip())                     \n",
    "                            current_chunk.extend([list_chunk])\n",
    "                            page_chunks.append(current_chunk)\n",
    "                            entire_list_chunk.append([list_chunk])\n",
    "\n",
    "                        num_words=0\n",
    "                        current_chunk=[]\n",
    "                        curr_chunk = []\n",
    "\n",
    "                if curr_chunk  and lines.index(line) == len(lines)-1: \n",
    "                    list_chunk=\"\\n\".join(curr_chunk)\n",
    "                    if header: \n",
    "                        if current_chunk and current_chunk[-1].strip().lower()==header.strip().lower():\n",
    "                            current_chunk.pop(-1)                       \n",
    "                        if TITLE and TITLE.lower().strip() != header.lower().strip():\n",
    "                            if first_page_portion:\n",
    "                                first_page_portion=False\n",
    "                            else:\n",
    "                                current_chunk.insert(0, TITLE.strip())                \n",
    "                        current_chunk.extend([header.strip()+':' if not header.strip().endswith(':') else header.strip() ]+[list_chunk])\n",
    "                        page_chunks.append(current_chunk)\n",
    "                        entire_list_chunk.append([header]+[list_chunk])\n",
    "                    else:\n",
    "                        if TITLE:\n",
    "                            if first_page_portion:\n",
    "                                first_page_portion=False\n",
    "                            else:\n",
    "                                current_chunk.insert(0, TITLE.strip())                \n",
    "                        current_chunk.extend([list_chunk])\n",
    "                        page_chunks.append(current_chunk)\n",
    "                        entire_list_chunk.append([list_chunk])\n",
    "                    num_words=0\n",
    "                    current_chunk=[]\n",
    "                elif curr_chunk and lines.index(line) != len(lines)-1:      \n",
    "                    list_chunk=\"\\n\".join(curr_chunk)\n",
    "                    if header:               \n",
    "                        if current_chunk and current_chunk[-1].strip().lower()==header.strip().lower():\n",
    "                            current_chunk.pop(-1) \n",
    "                        current_chunk.extend([header.strip()+':' if not header.strip().endswith(':') else header.strip() ]+[list_chunk])\n",
    "                    else:\n",
    "                        current_chunk.extend([list_chunk])                  \n",
    "                    num_words=words\n",
    "\n",
    "\n",
    "    if current_chunk:\n",
    "        if TITLE:\n",
    "            if first_page_portion:\n",
    "                first_page_portion=False\n",
    "            else:\n",
    "                current_chunk.insert(0, TITLE.strip())\n",
    "            # current_chunk.insert(0, TITLE.strip())\n",
    "        page_chunks.append(current_chunk)\n",
    "        entire_list_chunk.append(current_chunk)\n",
    "        current_chunk=[]\n",
    "    chunks[page] = page_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9501b756-ba00-48f7-a400-49bae1d834d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i, chunk in enumerate(chunks[59], start=1):\n",
    "    print(f'Chunk {i}:')\n",
    "    for item in chunk:\n",
    "        print(item)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5f93e9-4813-4b4e-a040-87e2370ec64b",
   "metadata": {},
   "source": [
    "This cell creates a domain in Amazon OpenSearch Service (Provisioned Capacity) and indexes the document chunks. The index mapping includes metadata fields for document names, page, and tables.\\\n",
    "The table field will be populated with the full tables in each page for each chunk in that page. This will serve as additional information to augment each page chunk when retrieved.\\\n",
    "We make use of an embedding model to create the embeddings and index them, the two options provided by this implementation are `all-mini-lmv6` and `titan` if using the former do deploy the sagemaker endpoint and provide the name in the utility function on cell 2 above.\\\n",
    "**Note** Some chunks may exceed the threshold set for chunking in the previous cells due to the way tables are chunked by row which may exceed the threshold. If run into a `token limit exceed` error while creating embeddings (ran into this issue with cohere embedding model on Bedrock), do use a larger sequence length embedding model. \\\n",
    "Replace the **domain_endpoint** variable with the one you created in your account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f459d61-102c-44c4-8b65-245b74c5d883",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pip install requests-aws4auth\n",
    "# pip install opensearch-py\n",
    "from opensearchpy import OpenSearch, RequestsHttpConnection\n",
    "from requests_aws4auth import AWS4Auth\n",
    "\n",
    "\"\"\"\n",
    "This is Amazon opensearch domain that uses IAM for authetication\n",
    "\"\"\"\n",
    "# Embedding Model\n",
    "model=\"all-mini-lm\" # other option \"titan\" #you can also use any other model in Bedrock or SageMaker Jumpstart or HuggingFace\n",
    "\n",
    "domain_endpoint = \"OPENSEARCH SEVICE DOMAIN NAME\"\n",
    "service = 'es'\n",
    "credentials = boto3.Session().get_credentials()\n",
    "awsauth = AWS4Auth(credentials.access_key, credentials.secret_key, \"us-east-1\", service, session_token=credentials.token)\n",
    "os_ = OpenSearch(\n",
    "    hosts = [{'host': domain_endpoint, 'port': 443}],\n",
    "    http_auth = awsauth,\n",
    "    use_ssl = True,\n",
    "    verify_certs = True,\n",
    "    timeout=120,        \n",
    "    # http_compress = True, # enables gzip compression for request bodies\n",
    "    connection_class = RequestsHttpConnection\n",
    ")\n",
    "\n",
    "# Sample Opensearch domain index mapping\n",
    "mapping = {\n",
    "  'settings': {\n",
    "    'index': {  \n",
    "      'knn': True,\n",
    "      \"knn.algo_param.ef_search\": 100,            \n",
    "    }\n",
    "      },\n",
    "\n",
    "      'mappings': {  \n",
    "        'properties': {\n",
    "          'embedding': {\n",
    "            'type': 'knn_vector', \n",
    "            'dimension':384 if \"all-mini-lm\" in model else (1536 if \"titan\" in model else None), #change as per sequence length of Embedding Model\n",
    "            \"method\": {\n",
    "              \"name\": \"hnsw\",       \n",
    "              \"space_type\": \"innerproduct\",\n",
    "              \"engine\": \"nmslib\",\n",
    "              \"parameters\": {\n",
    "                 \"ef_construction\": 256,\n",
    "                 \"m\":  48\n",
    "               }\n",
    "            }\n",
    "          },\n",
    "\n",
    "          'passage_id': {\n",
    "            'type': 'keyword'\n",
    "          },\n",
    "\n",
    "          'passage': {\n",
    "            'type': 'text'\n",
    "          },\n",
    "\n",
    "          'doc_id': {\n",
    "            'type': 'keyword'\n",
    "          },\n",
    "        \n",
    "          'table': {\n",
    "            'type': 'text'\n",
    "          },\n",
    "\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "\n",
    "domain_index = f\"financials\"    \n",
    "\n",
    "if not os_.indices.exists(index=domain_index):        \n",
    "    os_.indices.create(index=domain_index, body=mapping)\n",
    "    # Verify that the index has been created\n",
    "    if os_.indices.exists(index=domain_index):\n",
    "        print(f\"Index {domain_index} created successfully.\")\n",
    "    else:\n",
    "        print(f\"Failed to create index '{domain_index}'.\")\n",
    "else:\n",
    "    print(f'{domain_index} Index already exists!')\n",
    "\n",
    "i = 1\n",
    "import boto3\n",
    "SAGEMAKER=boto3.client('sagemaker-runtime')\n",
    "for page, chunkks in chunks.items(): \n",
    "    chunk_id = page \n",
    "   \n",
    "    \n",
    "    for chunk in chunkks:\n",
    "        passage_chunk=\"\\n\".join(chunk)        \n",
    "        embedding=_get_emb_(passage_chunk, model)     \n",
    "        table=[]\n",
    "        if page in table_page:\n",
    "            for ids,item in enumerate(table_page[page]):\n",
    "                header=table_header_dict[page][ids]\n",
    "                if header.strip():\n",
    "                    header=f\"<table_header>{header}</table_header>\"\n",
    "                tsv=f\"<table>{item}</table>\"\n",
    "                table.append(header)\n",
    "                table.append(tsv)\n",
    "            table=\"\\n\".join(table)                \n",
    "        documentt = { \n",
    "            'doc_id':doc_id,\n",
    "            'passage_id': chunk_id, \n",
    "            'passage': passage_chunk, \n",
    "            'embedding': embedding,\n",
    "            'table':table\n",
    "        }\n",
    "        try:\n",
    "            response = os_.index(index=domain_index, body=documentt)\n",
    "            i += 1\n",
    "           \n",
    "            if response[\"result\"] == \"created\":\n",
    "                print(f\"Document indexed successfully with ID: {response['_id']}\")\n",
    "            else:\n",
    "                print(\"Failed to index document.\")\n",
    "        except RequestError as e:\n",
    "            logging.error(f\"Error indexing document to index '{domain_index}': {e}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb7c3cc-beb5-48e2-abaa-66b46c5e8373",
   "metadata": {},
   "source": [
    "# RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1bbeba-2fd9-4775-aec2-8a8c287e8655",
   "metadata": {},
   "source": [
    "## Bedrock Anthropic LLM Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16e8791-8b4f-4999-b42a-b89d3a7d3d01",
   "metadata": {},
   "source": [
    "The following cells perform RAG on the newly created index. We run a sample question to test the implementation above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3f7c64-1ddb-4c4a-a050-67669632790b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "question=\"GHG reporting metrics and targets for 2021 compared to 2022?\"\n",
    "embedding=_get_emb_(question, model) \n",
    "query = {\n",
    "    'size': 3,\n",
    "    'query': {\n",
    "        \"knn\": {\n",
    "          \"embedding\": {\n",
    "            \"vector\": embedding,\n",
    "            \"k\": 3\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "response = os_.search(index=domain_index, body=query)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3a127e-74c4-4500-a48d-084d026108a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "res=response['hits']['hits']\n",
    "score = [str(x['_score']) for x in res] \n",
    "passage = [x['_source']['passage'] for x in res] \n",
    "page_no = [x['_source']['passage_id'] for x in res] \n",
    "doc_name = [x['_source']['doc_id'] for x in res]\n",
    "tables=[x['_source']['table'] for x in res] \n",
    "p = inflect.engine()\n",
    "## Concatenate passages and tables\n",
    "passages=\"\"\n",
    "tab=\"\"\n",
    "for  ids,text in enumerate(passage):\n",
    "    passages+=f\"<{p.ordinal(ids+1)}_passage>\\n{text}\\n</{p.ordinal(ids+1)}_passage>\\n\"\n",
    "    tab+=f\"<{p.ordinal(ids+1)}_passage_table>\\n{tables[ids]}\\n</{p.ordinal(ids+1)}_passage_table>\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc0c8ab-5cf5-4200-829f-08c576db2d45",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt_template=f\"\"\"You are a helpful, obedient and truthful financial assistance.\n",
    "\n",
    "<document>\n",
    "{passages}\n",
    "</document>          \n",
    "\n",
    "<additional_information>\n",
    "{tab}\n",
    "</additional_information>\n",
    "\n",
    "<instructions>\n",
    "When providing your response based on the document:\n",
    "1. Understand the question to know what is being asked of you.\n",
    "2. Review the entire document provided and check if it contains relevant information to answer the question. Only pay attention to passages with relevant information.\n",
    "3. If the document is sufficient to answer the question, provide a comprehensive answer ENTIRELY based on the document provided. DO NOT make up answers not present in the document.\n",
    "4. If the answer is not available in the document, say so.\n",
    "</instructions>\n",
    "\n",
    "Question: {question}\"\"\"\n",
    "prompt=f\"\\n\\nHuman:{prompt_template}\\n\\nAssistant: Based on the document,\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063e1c51-508c-4f5b-8928-b98a2a10e2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_model='anthropic.claude-v2'\n",
    "inference_modifier = {'max_tokens_to_sample':300, \n",
    "                      \"temperature\":0.1,\n",
    "                      # \"top_k\":250,\n",
    "                      # \"top_p\":1,               \n",
    "                     }\n",
    "llm = Bedrock(model_id=txt_model, client=bedrock_runtime, model_kwargs = inference_modifier,\n",
    "              streaming=True,  # Toggle this to turn streaming on or off\n",
    "              callbacks=[StreamingStdOutCallbackHandler() ])\n",
    "\n",
    "response = llm(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16fe1cf-998b-4469-bf08-e9b77feead7b",
   "metadata": {},
   "source": [
    "## SageMaker JumpStart Mixtral 8x7b Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81104193-61c8-481e-ae83-0dd36fa46b87",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt=f\"\"\"<s><<SYS>>[INST]\n",
    "You are a helpful, obedient and truthful assistant. You will only provide answers entirely based on the document provided below.\n",
    "\n",
    "Here is a document:\n",
    "####\n",
    "{passages}\n",
    "####\n",
    "\n",
    "Here is additional information:\n",
    "####\n",
    "{tab}\n",
    "####\n",
    "\n",
    "When providing your response based on the provided document:\n",
    "1. Understand the question to know what is being asked of you.\n",
    "2. Review the entire document provided and check if it contains relevant information to answer the question. Only pay attention to sections with relevant information.\n",
    "3. If the document is sufficient to answer the question, provide a comprehensive answer ENTIRELY based on the document provided. DO NOT make up answers not present in the document.\n",
    "4. If the answer is not available in the document, say so.<</SYS>>\n",
    "\n",
    "Question: {question}[/INST]\n",
    "Answer: According to the document provided,\"\"\"\n",
    "parameters = { \"max_new_tokens\": 300,       \n",
    "        \"return_full_text\": False,\n",
    "        \"temperature\": 0.1}\n",
    "mixtral_instruct.model_kwargs = parameters\n",
    "print(mixtral_instruct(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c2cb09-5ef4-41dc-bbb4-05d235423f32",
   "metadata": {},
   "source": [
    "# SUMMARIZATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bce4526-8167-45bd-a0fd-dfe0ee1ba0ad",
   "metadata": {},
   "source": [
    "## Bedrock Athropic Claude"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8cebab1-86e6-4338-99e3-7c5914086405",
   "metadata": {},
   "source": [
    "The following cell does a full document summarization taking advantage of the tagged section to further guide the LLM response. This does not use RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3271d6-436f-47de-90b7-8fd37aad8121",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "doc=\"\"\n",
    "for k,v in document_holder.items():\n",
    "    doc+=\"\\n\".join(v)\n",
    "# print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cffad7b6-d912-490a-b44e-53af7792a018",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt_template=f\"\"\"You are a financial analyst, great at writing summaries of company financial performance.\n",
    "\n",
    "Here is a document:\n",
    "<document>\n",
    "{doc}\n",
    "</document>\n",
    "\n",
    "Before responding:\n",
    "1. Read the document several times to ensure you understand everything.\n",
    "2. Take notes to identify the relevant information.\n",
    "\n",
    "Provide a comprehensive summary of the document.\n",
    "Your summary should include the following if available in the document:\n",
    "1. Revenue and Profitability\n",
    "2. Cash Flow\n",
    "3. Debt and Solvency\n",
    "4. Market and Industry Trends\n",
    "5. Future Outlook\n",
    "6. Risks and Contingencies\n",
    "\n",
    "Base your entire summary on the information provided in the document.\"\"\"\n",
    "prompt=f\"\\n\\nHuman:{prompt_template}\\n\\nAssistant:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8d9540-2573-44be-9fac-0ec7228df875",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "txt_model='anthropic.claude-v2'\n",
    "inference_modifier = {'max_tokens_to_sample':1000, \n",
    "                      \"temperature\":0.1,\n",
    "                      # \"top_k\":250,\n",
    "                      # \"top_p\":1,               \n",
    "                     }\n",
    "llm = Bedrock(model_id=txt_model, client=bedrock_runtime, model_kwargs = inference_modifier,\n",
    "              streaming=True,  # Toggle this to turn streaming on or off\n",
    "              callbacks=[StreamingStdOutCallbackHandler() ])\n",
    "\n",
    "response = llm(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f080bd-138d-4777-91bf-fb7e4b38cf13",
   "metadata": {},
   "source": [
    "## SageMaker JumpStart Mixtral 8x7b "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d36532-8418-45dd-a9c7-9fd86851cf3e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "map_custom_prompt=\"\"\"<s><<SYS>>[INST]\n",
    "You are a financial analyst, great at writing summaries of company financial performance.\n",
    "\n",
    "Here is a document:\n",
    "######\n",
    "`{text}`\n",
    "######\n",
    "\n",
    "Before responding:\n",
    "1. Read the document several times to ensure you understand everything.\n",
    "2. Take notes to identify the relevant information.<</SYS>>\n",
    "\n",
    "Provide a comprehensive summary of the document.\n",
    "Your summary should include the following if available in the document:\n",
    "1. Revenue and Profitability\n",
    "2. Cash Flow\n",
    "3. Debt and Solvency\n",
    "4. Market and Industry Trends\n",
    "5. Future Outlook\n",
    "6. Risks and Contingencies\n",
    "Base your entire summary on the information provided in the document.[/INST]\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6a19d0-db1b-40ec-a71f-04a29b8212f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "combine_custom_prompt=\"\"\"<s><<SYS>>[INST]\n",
    "You are a financial analyst, great at writing summaries of company financial performance.\n",
    "\n",
    "Here are multiple summaries of various parts of a documnet:\n",
    "######\n",
    "`{text}`\n",
    "######\n",
    "<</SYS>>\n",
    "Provide a coherent final summary that connects your individual summaries perfectly.\n",
    "Your final summary should follow the same outline:\n",
    "1. Revenue and Profitability\n",
    "2. Cash Flow\n",
    "3. Debt and Solvency\n",
    "4. Market and Industry Trends\n",
    "5. Future Outlook\n",
    "6. Risks and Contingencies\n",
    "[/INST]\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323a334e-b3c0-411f-871d-e61e776f9b9d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=5000, chunk_overlap=100,length_function=len)\n",
    "\n",
    "texts = text_splitter.create_documents([doc])\n",
    "print(len(texts))\n",
    "\n",
    "parameters = { \"max_new_tokens\": 500, }\n",
    "mixtral_instruct.model_kwargs = parameters\n",
    "\n",
    "combine_prompt_template = PromptTemplate(\n",
    "    template=combine_custom_prompt, \n",
    "    input_variables=['text']\n",
    ")\n",
    "\n",
    "map_prompt_template = PromptTemplate (\n",
    "    input_variables=['text'],\n",
    "    template=map_custom_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eee90cc-a28d-40b8-b968-f5f0dc6def63",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "chain = load_summarize_chain(mixtral_instruct, \n",
    "                             chain_type=\"map_reduce\", \n",
    "                             verbose=True,\n",
    "                             map_prompt=map_prompt_template,\n",
    "                            combine_prompt=combine_prompt_template,\n",
    "                            )\n",
    "chain.run(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c026e000-2dcf-4b98-aa4c-1cbeb2a1d0ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.c5.large",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
