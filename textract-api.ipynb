{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa108161-6352-4d2d-b2c9-cad24879b579",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install --force-reinstall amazon-textract-textractor==1.7.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "id": "5d14636c-4f02-4b2d-874a-cc44de0c85c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "from textractor import Textractor\n",
    "from textractor.visualizers.entitylist import EntityList\n",
    "from textractor.data.constants import TextractFeatures\n",
    "\n",
    "import boto3\n",
    "from botocore.config import Config\n",
    "\n",
    "# Create the bedrock runtime to invoke LLM\n",
    "from botocore.config import Config\n",
    "config = Config(\n",
    "    read_timeout=600, #this timeout determines the maximum time (secs) allowed for the client to wait for data to be received from the server. \n",
    "    retries = dict(\n",
    "        max_attempts = 5 ## maximum number of retry attempts that will be made on a single request\n",
    "    )\n",
    ")\n",
    "import boto3\n",
    "bedrock_runtime = boto3.client(service_name='bedrock-runtime',region_name='us-east-1',config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8144b0ae-4209-4a0d-aa71-6fa8097d1653",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bedrock_streemer(response,model):\n",
    "    stream = response.get('body')\n",
    "    answer = \"\"\n",
    "    i = 1\n",
    "    if stream:\n",
    "        for event in stream:\n",
    "            chunk = event.get('chunk')\n",
    "            if chunk:\n",
    "                chunk_obj = json.loads(chunk.get('bytes').decode())\n",
    "                if 'claude' in model.lower():\n",
    "                    text = chunk_obj['completion']\n",
    "                if 'titan' in model.lower():\n",
    "                    text = chunk_obj[\"outputText\"]\n",
    "                if 'cohere' in model.lower():\n",
    "                    text = chunk_obj[\"generations\"][0]['text']\n",
    "                if 'llama2' in model.lower():\n",
    "                    text = chunk_obj[\"generation\"]\n",
    "                answer+=text\n",
    "                print(text, end=\"\")             \n",
    "                i+=1\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d353848f-6a67-4f7a-b6fa-0a062f61457c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LineIterator:\n",
    "    \"\"\"\n",
    "    A helper class for parsing the byte stream input. \n",
    "    \n",
    "    The output of the model will be in the following format:\n",
    "    ```\n",
    "    b'{\"outputs\": [\" a\"]}\\n'\n",
    "    b'{\"outputs\": [\" challenging\"]}\\n'\n",
    "    b'{\"outputs\": [\" problem\"]}\\n'\n",
    "    ...\n",
    "    ```\n",
    "    \n",
    "    While usually each PayloadPart event from the event stream will contain a byte array \n",
    "    with a full json, this is not guaranteed and some of the json objects may be split across\n",
    "    PayloadPart events. For example:\n",
    "    ```\n",
    "    {'PayloadPart': {'Bytes': b'{\"outputs\": '}}\n",
    "    {'PayloadPart': {'Bytes': b'[\" problem\"]}\\n'}}\n",
    "    ```\n",
    "    \n",
    "    This class accounts for this by concatenating bytes written via the 'write' function\n",
    "    and then exposing a method which will return lines (ending with a '\\n' character) within\n",
    "    the buffer via the 'scan_lines' function. It maintains the position of the last read \n",
    "    position to ensure that previous bytes are not exposed again. \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, stream):\n",
    "        self.byte_iterator = iter(stream)\n",
    "        self.buffer = io.BytesIO()\n",
    "        self.read_pos = 0\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        while True:\n",
    "            self.buffer.seek(self.read_pos)\n",
    "            line = self.buffer.readline()\n",
    "            if line and line[-1] == ord('\\n'):\n",
    "                self.read_pos += len(line)\n",
    "                return line[:-1]\n",
    "            try:\n",
    "                chunk = next(self.byte_iterator)\n",
    "            except StopIteration:\n",
    "                if self.read_pos < self.buffer.getbuffer().nbytes:\n",
    "                    continue\n",
    "                raise\n",
    "            if 'PayloadPart' not in chunk:\n",
    "                print('Unknown event type:' + chunk)\n",
    "                continue\n",
    "            self.buffer.seek(0, io.SEEK_END)\n",
    "            self.buffer.write(chunk['PayloadPart']['Bytes'])\n",
    "            \n",
    "def jumpstart_streemer(output):\n",
    "    answer=\"\"\n",
    "    stop_token = '<|endoftext|>'\n",
    "    event_stream = output['Body']\n",
    "    start_json = b'{'\n",
    "    for line in LineIterator(event_stream):\n",
    "        if line != b'' and start_json in line:\n",
    "            data = json.loads(line[line.find(start_json):].decode('utf-8'))\n",
    "            if data['token']['text'] != stop_token:\n",
    "                answer+=data['token']['text']\n",
    "                print(data['token']['text'], end=\"\")  \n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ac6822-8ebb-411e-ba31-a31e27383604",
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp amzn-20221231.pdf s3://BUCKET-NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "id": "b11f468c-bd49-488e-834c-7ec037d4a6a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "extractor = Textractor(region_name=\"us-east-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "id": "5c1d6899-82bd-49aa-8d64-ff4ebfdb235b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file=\"s3://BUCKET/amzn-20221231.pdf\"\n",
    "doc_id=file_name = os.path.basename(file)\n",
    "\n",
    "document = extractor.start_document_analysis(\n",
    "    file_source=file,\n",
    "    features=[TextractFeatures.LAYOUT,TextractFeatures.TABLES],\n",
    "    client_request_token=doc_id.split('.')[0],\n",
    "    save_image=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "id": "1f086761-25bd-4535-b1b1-10724abceae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Intangible Assets \n",
      "\n",
      "Acquired identifiable intangible assets are valued primarily by using discounted cash flows. These assets are included within \"Other assets\" on our consolidated balance sheets and consist of the following (in millions): \n",
      "\n",
      "\n",
      "\n",
      "<tables><table>\tDecember 31, \t\t\t\t\t\t\n",
      "\t2021 \t\t\t2022 \t\t\t\n",
      "\tAcquired Intangibles, Gross (1) \tAccumulated Amortization (1) \tAcquired Intangibles, Net \tAcquired Intangibles, Gross (1) \tAccumulated Amortization (1) \tAcquired Intangibles, Net \tWeighted Average Life Remaining \n",
      "Finite-lived intangible assets (2): \t\t\t\t\t\t\t\n",
      "Marketing-related \t$ 2,286 \t$ (548) \t$ 1,738 \t$ 2,407 \t$ (601) \t$ 1,806 \t18.6 \n",
      "Contract-based \t2,327 \t(565) \t1,762 \t3,661 \t(813) \t2,848 \t12.8 \n",
      "Technology- and content-based \t976 \t(610) \t366 \t883 \t(643) \t240 \t3.2 \n",
      "Customer-related \t197 \t(103) \t94 \t184 \t(128) \t56 \t2.2 \n",
      "Total finite-lived intangible assets \t$ 5,786 \t$ (1,826) \t$ 3,960 \t$ 7,135 \t$ (2,185) \t$ 4,950 \t14.4 \n",
      "IPR&D and other (3) \t$ 1,147 \t\t$ 1,147 \t$ 1,147 \t\t$ 1,147 \t\n",
      "Total acquired intangibles \t$ 6,933 \t$ (1,826) \t$ 5,107 \t$ 8,282 \t$ (2,185) \t$ 6,097 \t\n",
      "</table>\n",
      "\n",
      "\n",
      "\n",
      "<list>(1) Excludes the original cost and accumulated amortization of fully-amortized intangibles. \n",
      "(2) Finite-lived intangible assets, excluding acquired video content, have estimated useful lives of between one and twenty-five years, and are being amortized to operating expenses on a straight-line basis. \n",
      "(3) Intangible assets acquired in a business combination that are in-process and used in research and development activities are considered indefinite-lived until the completion or abandonment of the research and development efforts. Once the research and development efforts are completed, we determine the useful life and begin amortizing the assets. </list>\n",
      "\n",
      "Amortization expense for acquired finite-lived intangibles was $509 million, $512 million, and $604 million in 2020, 2021, and 2022. Expected future amortization expense of acquired finite-lived intangible assets as of December 31, 2022 is as follows (in millions): \n",
      "\n",
      "Year Ended December 31, \n",
      "\n",
      "\n",
      "\n",
      "<tables><table>2023 \t$ 530 \n",
      "2024 \t456 \n",
      "2025 \t371 \n",
      "2026 \t324 \n",
      "2027 \t314 \n",
      "Thereafter \t2,955 \n",
      "\t$ 4,950 \n",
      "</table>\n",
      "\n",
      "\n",
      "\n",
      "55 \n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from textractor.data.text_linearization_config import TextLinearizationConfig\n",
    "\n",
    "config = TextLinearizationConfig(\n",
    "    hide_figure_layout=True,\n",
    "    title_prefix=\"<title> \",\n",
    "     title_suffix=\"</title> \",\n",
    "    hide_header_layout=True,\n",
    "    # section_header_prefix=\"<header>\",\n",
    "    # section_header_suffix=\"</header>\",\n",
    "    # table_linearization_format=\"markdown\", \n",
    "    table_prefix=\"<tables><table>\",\n",
    "    table_suffix=\"</table>\",\n",
    "# table_layout_prefix=\"<table_layout>\",\n",
    "# table_layout_suffix=\"</table_layout>\",\n",
    "    # table_column_separator=\"|\",\n",
    "    # table_tabulate_format = 'github',\n",
    "    list_layout_prefix=\"<list>\",\n",
    "    list_layout_suffix=\"</list>\",\n",
    "    hide_footer_layout=True,\n",
    "    hide_page_num_layout=True,\n",
    "    # list_element_prefix=\"&&&\",\n",
    "    # list_element_suffix=\"%%%%\"\n",
    ")\n",
    "\n",
    "print(document.pages[84].get_text(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "id": "5a1806be-76d0-42b1-b064-70a95cc7daf3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def split_list_items_(items):\n",
    "    parts = re.split(\"(<list>|</list>)\", items)  \n",
    "    output = []\n",
    "\n",
    "    inside_list = False\n",
    "    list_item = \"\"\n",
    "\n",
    "    for p in parts:\n",
    "        if p == \"<list>\":\n",
    "            inside_list = True\n",
    "            # output.append(p)\n",
    "            list_item=p\n",
    "        elif p == \"</list>\":\n",
    "            inside_list = False\n",
    "            list_item += p\n",
    "            output.append(list_item)\n",
    "            list_item = \"\" \n",
    "        elif inside_list:\n",
    "            list_item += p.strip()\n",
    "        else:\n",
    "            output.extend(p.split('\\n'))\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "id": "a011ade1-bd57-49e3-a531-297e3c34d4f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "We filter through each page with a table and replace that free text table with a csv formatted table \n",
    "demarcated by <table> xml tags\n",
    "\"\"\"\n",
    "csv_seperator=\"|\"\n",
    "document_holder={}\n",
    "table_page={}\n",
    "count=0\n",
    "for ids,page in enumerate(document.pages):\n",
    "\n",
    "    table_count=len([word for word in page.get_text(config=config).split() if \"<tables><table>\" in word]) # get the number of table in the extracted document page by header we set earlier\n",
    "    assert table_count==len(page.tables) # check that number of tables per page is same as tables extracted by textract TABLE feature\n",
    "    content=page.get_text(config=config).split(\"<tables>\")\n",
    "    document_holder[ids]=[]    \n",
    "    for idx,item in enumerate(content):\n",
    "        if \"<table>\" in item:     \n",
    "\n",
    "            df0=document.tables[count].to_pandas(use_columns=False).to_csv(header=False, index=None,sep=csv_seperator)\n",
    "            row_count=len([x for x in df0.split(\"\\n\") if x]) #Check the number of rows in the parsed table to determine how to read the table headers. if table row count is 1 then headers is obviously at 0 else headers may or may not be at 0\n",
    "            if row_count>1:\n",
    "                if not all(value.strip() == '' for value in df0.split(\"\\n\")[0].split(csv_seperator)): #Check if the first row in the csv is empty headers due to the way Textract parses the csv at times (with empty column headers while true headers are at index=1)\n",
    "                    row_count=1 # if true column headers sit at row index 0 set row_count=1 else true column headers sit at row index 1\n",
    "            df=pd.read_csv(io.StringIO(df0), sep=csv_seperator, \n",
    "                           header=0 if row_count==1 else 1, keep_default_na=False) # read table with appropiate column headers\n",
    "            df.rename(columns=lambda x: '' if x.startswith('Unnamed:') else x, inplace=True) # replace pandas \"unnamed\" placeholder for empty column names with empty string\n",
    "            table=df.to_csv(index=None, sep=csv_seperator)\n",
    "            if ids in table_page:\n",
    "                table_page[ids].append(table)\n",
    "            else:\n",
    "                table_page[ids]=[table]\n",
    "            pattern = re.compile(r'<table>(.*?)(</table>)', re.DOTALL)  ## scoop the table csv string from other non-tabular text\n",
    "            data=item\n",
    "            table_match = re.search(pattern, data)\n",
    "            table_data = table_match.group(1) if table_match else ''  ## table\n",
    "            remaining_content = data[table_match.end():] if table_match else data ## non-tabular text\n",
    "            content[idx]=f\"<table>{table}</table>\" ## attach xml tags to differentiate table from other text\n",
    "            count+=1\n",
    "            if \"<list>\" in remaining_content: # keep list item as a single item in the python list\n",
    "                output=split_list_items_(remaining_content)\n",
    "                output=[x.strip() for x in output if x.strip()]\n",
    "                document_holder[ids].extend([content[idx]]+output)           \n",
    "            else:\n",
    "                document_holder[ids].extend([content[idx]]+[x.strip() for x in remaining_content.split('\\n') if x.strip()]) # split other text by new line to be independent items in the python list.\n",
    "        else:   \n",
    "            if \"<list>\" in item and \"<table>\" not in item:   \n",
    "                output=split_list_items_(item)\n",
    "                output=[x.strip() for x in output if x.strip()]\n",
    "                document_holder[ids].extend(output)\n",
    "            else:\n",
    "                document_holder[ids].extend([x.strip() for x in item.split(\"\\n\") if x.strip()])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "id": "f279709f-fe7a-4065-94c7-5efeeab8d0d4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intangible Assets\n",
      "Acquired identifiable intangible assets are valued primarily by using discounted cash flows. These assets are included within \"Other assets\" on our consolidated balance sheets and consist of the following (in millions):\n",
      "<table>|December 31, ||||||\n",
      "|2021 |||2022 |||\n",
      "|Acquired Intangibles, Gross (1) |Accumulated Amortization (1) |Acquired Intangibles, Net |Acquired Intangibles, Gross (1) |Accumulated Amortization (1) |Acquired Intangibles, Net |Weighted Average Life Remaining \n",
      "Finite-lived intangible assets (2): |||||||\n",
      "Marketing-related |$ 2,286 |$ (548) |$ 1,738 |$ 2,407 |$ (601) |$ 1,806 |18.6 \n",
      "Contract-based |2,327 |(565) |1,762 |3,661 |(813) |2,848 |12.8 \n",
      "Technology- and content-based |976 |(610) |366 |883 |(643) |240 |3.2 \n",
      "Customer-related |197 |(103) |94 |184 |(128) |56 |2.2 \n",
      "Total finite-lived intangible assets |$ 5,786 |$ (1,826) |$ 3,960 |$ 7,135 |$ (2,185) |$ 4,950 |14.4 \n",
      "IPR&D and other (3) |$ 1,147 ||$ 1,147 |$ 1,147 ||$ 1,147 |\n",
      "Total acquired intangibles |$ 6,933 |$ (1,826) |$ 5,107 |$ 8,282 |$ (2,185) |$ 6,097 |\n",
      "</table>\n",
      "<list>(1) Excludes the original cost and accumulated amortization of fully-amortized intangibles. \n",
      "(2) Finite-lived intangible assets, excluding acquired video content, have estimated useful lives of between one and twenty-five years, and are being amortized to operating expenses on a straight-line basis. \n",
      "(3) Intangible assets acquired in a business combination that are in-process and used in research and development activities are considered indefinite-lived until the completion or abandonment of the research and development efforts. Once the research and development efforts are completed, we determine the useful life and begin amortizing the assets.</list>\n",
      "Amortization expense for acquired finite-lived intangibles was $509 million, $512 million, and $604 million in 2020, 2021, and 2022. Expected future amortization expense of acquired finite-lived intangible assets as of December 31, 2022 is as follows (in millions):\n",
      "Year Ended December 31,\n",
      "<table>2023 |$ 530 \n",
      "2024 |456 \n",
      "2025 |371 \n",
      "2026 |324 \n",
      "2027 |314 \n",
      "Thereafter |2,955 \n",
      "|$ 4,950 \n",
      "</table>\n",
      "55\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\".join(document_holder[84]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "id": "919c8154-00b5-466b-b3f6-54f91cde19fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "\n",
    "max_words = 200\n",
    "chunks = {}\n",
    "entire_list_chunk=[]\n",
    "table_header_dict={} ## hold table headers to be used for chunk indexing\n",
    "list_header_dict={}\n",
    "overlap=50\n",
    "for page, lines in document_holder.items():\n",
    "    page_chunks = []\n",
    "    current_chunk = []\n",
    "    num_words = 0   \n",
    "    start_page=0\n",
    "    TITLE=None\n",
    "    for line in lines:\n",
    "        if line.strip()!= '':\n",
    "            # COmment this code block out if you dont want to include topic chunkimg\n",
    "            if \"<title>\" in line:   \n",
    "                TITLE=line.split(\"<title>\")[-1].split(\"</title>\")[0]\n",
    "                line=TITLE\n",
    "                TITLE=TITLE.upper()               \n",
    "            \n",
    "\n",
    "            if len(current_chunk)<2 and entire_list_chunk and start_page==0:    \n",
    "                cumulative_word_count = 0\n",
    "                items_within_threshold = 0\n",
    "                \n",
    "                # Iterate through the list from the bottom up and add up to 50 words from previous page\n",
    "                for item in reversed(entire_list_chunk[-1]):\n",
    "                    words = item.split()\n",
    "                    if cumulative_word_count + len(words) <= overlap:\n",
    "                        cumulative_word_count += len(words)\n",
    "                        items_within_threshold += 1\n",
    "                        current_chunk.insert(0, item)  # Insert at the beginning to maintain order\n",
    "                    else:\n",
    "                        break\n",
    "                first_page_portion=True\n",
    "                num_words=cumulative_word_count\n",
    "                start_page=1\n",
    "\n",
    "            next_num_words = num_words + len(re.findall(r'\\w+', line))  \n",
    "\n",
    "            if  \"<table>\" not in line and \"<list>\" not in line:\n",
    "                \n",
    "                \n",
    "                if next_num_words > max_words:\n",
    "                    if TITLE :\n",
    "                        if first_page_portion:\n",
    "                            first_page_portion=False\n",
    "                        else:\n",
    "                            current_chunk.insert(0, TITLE.strip())\n",
    "                        \n",
    "                    page_chunks.append(current_chunk)\n",
    "                    entire_list_chunk.append(current_chunk)\n",
    "                    current_chunk = []\n",
    "                    num_words = 0\n",
    "\n",
    "                current_chunk.append(line)    \n",
    "                num_words += len(re.findall(r'\\w+', line))\n",
    "                \n",
    "            \"\"\"\n",
    "            Goal is to segment out table items and chunks intelligently.\n",
    "            We chunk the table by rows and for each chunk of the table we append the table column headers\n",
    "            and table headers if any. This way we preserve the table information across each chunks.\n",
    "            This will help improve semantic search where all the chunks relating to a table would be in the \n",
    "            top k=n response giving the LLM mcomplet information on the table.\n",
    "            \"\"\"\n",
    "\n",
    "\n",
    "            if \"<table>\" in line:\n",
    "                # Get table header which is usually line before table in document\n",
    "                line_index=lines.index(line)\n",
    "                if line_index!=0 and \"<table>\" not in lines[line_index-1] and \"<list>\" not in lines[line_index-1]: #Check if table is first item on the page, then they wont be a header (header may be included it table) and also if table is the the last item in the list\n",
    "                    header=lines[line_index-1]\n",
    "                else:\n",
    "                    header=\"\"\n",
    "                \n",
    "                if page in table_header_dict:\n",
    "                    table_header_dict[page].append(header)\n",
    "                else:\n",
    "                    table_header_dict[page]=[header]\n",
    "\n",
    "                table = line.split(\"<table>\")[-1].split(\"</table>\")[0] # get table from demarcators     \n",
    "                df=pd.read_csv(io.StringIO(table), sep=csv_seperator, keep_default_na=False)\n",
    "                df.rename(columns=lambda x: '' if x.startswith('Unnamed:') else x, inplace=True)\n",
    "\n",
    "                table_chunks = []\n",
    "                curr_chunk = [df.columns.to_list()] #start current chunk with table column names    \n",
    "                words=len(re.findall(r'\\w+', str(current_chunk)+\" \"+str(curr_chunk)))  \n",
    "                # Iterate through the rows in the table\n",
    "                for row in df.itertuples():\n",
    "                    curr_chunk.append(row)         \n",
    "                    words+=len(re.findall(r'\\w+', str(row)))#len(re.findall(r'\\w+', \" \".join([str(x) for x in row])))\n",
    "\n",
    "                    if words > max_words:\n",
    "                        # print(words,page, end=\"\\n\")                        \n",
    "                        table_chunks.append(\"\\n\".join([\"|\".join(str(x) for x in curr_chunk[0])] + [\"|\".join(str(x) for x in r) for r in curr_chunk[1:]])) #join chunk lines together to for a csv                   \n",
    "                        words = len(re.findall(r'\\w+', str(curr_chunk[0]))) # set word count to word length of column header names\n",
    "                        tab_chunk=\"\\n\".join([\"|\".join(str(x) for x in curr_chunk[0])] + [\"|\".join(str(x) for x in r) for r in curr_chunk[1:]]) #join chunk lines together to for a csv\n",
    "                        \n",
    "                        if header: #If header  attach header to table                         \n",
    "                            if current_chunk and current_chunk[-1]==header: #check if header is in the chunk and remove to avoid duplicacy of header in chunk                        \n",
    "                                current_chunk.pop(-1)\n",
    "                            \n",
    "                            header=header+\"\\n\" if not header.strip().endswith('\\n') else header  # add new line char to seperate from table  \n",
    "                            if TITLE:\n",
    "                                current_chunk.insert(0, TITLE.strip())\n",
    "                            current_chunk.extend([header]+[tab_chunk]) #if current_chunk and header!=current_chunk[-1] else current_chunk.extend([tab_chunk])\n",
    "                            page_chunks.append(current_chunk)                            \n",
    "                            entire_list_chunk.append([header]+table_chunks) #if current_chunk and header!=current_chunk[-1] else entire_list_chunk.append(table_chunks)\n",
    "                        else:\n",
    "                            if TITLE:\n",
    "                                current_chunk.insert(0, TITLE.strip())\n",
    "                            current_chunk.extend([tab_chunk])\n",
    "                            page_chunks.append(current_chunk)\n",
    "                            entire_list_chunk.append(table_chunks)\n",
    "                       \n",
    "                        num_words=0\n",
    "                        current_chunk=[]\n",
    "                        curr_chunk = [curr_chunk[0]]\n",
    "\n",
    "                if curr_chunk != [df.columns.to_list()] and lines.index(line) == len(lines)-1: #if table chunk still remaining and table is last item in page append as last chunk\n",
    "                    table_chunks.append(\"\\n\".join([\"|\".join(str(x) for x in curr_chunk[0])] + [\"|\".join(str(x) for x in r) for r in curr_chunk[1:]]))\n",
    "                    tab_chunk=\"\\n\".join([\"|\".join(str(x) for x in curr_chunk[0])] + [\"|\".join(str(x) for x in r) for r in curr_chunk[1:]])\n",
    "                    if header: \n",
    "                        if current_chunk and current_chunk[-1]==header: #check if header is in the chunk and remove to avoid duplicacy of header in chunk\n",
    "                            current_chunk.pop(-1)\n",
    "                        header=header+\"\\n\" if not header.strip().endswith('\\n') else header \n",
    "                        if TITLE:\n",
    "                            current_chunk.insert(0, TITLE.strip())\n",
    "                        current_chunk.extend([header]+[tab_chunk])\n",
    "                        page_chunks.append(current_chunk)\n",
    "                        entire_list_chunk.append([header]+table_chunks)\n",
    "                    else:\n",
    "                        if TITLE:\n",
    "                            current_chunk.insert(0, TITLE.strip())\n",
    "                        current_chunk.extend([tab_chunk])\n",
    "                        page_chunks.append(current_chunk)\n",
    "                        entire_list_chunk.append(table_chunks)\n",
    "                    num_words=0\n",
    "                    current_chunk=[]\n",
    "                elif curr_chunk != [df.columns.to_list()] and lines.index(line) != len(lines)-1: #if table is not last item in page and max word threshold is not reached, send no next loop\n",
    "                    table_chunks.append(\"\\n\".join([\"|\".join(str(x) for x in curr_chunk[0])] + [\"|\".join(str(x) for x in r) for r in curr_chunk[1:]]))\n",
    "                    tab_chunk=\"\\n\".join([\"|\".join(str(x) for x in curr_chunk[0])] + [\"|\".join(str(x) for x in r) for r in curr_chunk[1:]])\n",
    "                    if header:               \n",
    "                        if current_chunk and current_chunk[-1]==header: #check if header is in the chunk and remove to avoid duplicacy of header in chunk\n",
    "                            current_chunk.pop(-1)\n",
    "                        header=header+\"\\n\" if not header.strip().endswith('\\n') else header \n",
    "                        current_chunk.extend([header]+[tab_chunk])\n",
    "                    else:\n",
    "                        current_chunk.extend([tab_chunk])                  \n",
    "                    num_words=words\n",
    "            \n",
    "            \"\"\"\n",
    "            Goal is to segment out list items and chunk intelligently.\n",
    "            We chunk each list by items in the list and \n",
    "            for each list chunk we append the list header to the chunk to preserve the information of the list across chunks.\n",
    "            This would boost retrieval process where question pertaining to a list will have all list chunks within\n",
    "            the topK=n responses.\n",
    "            \"\"\"\n",
    "            \n",
    "            if \"<list>\" in line:\n",
    "                # Get list header which is usually line before list in document\n",
    "                line_index=lines.index(line)\n",
    "                if line_index!=0 and \"<table>\" not in lines[line_index-1] and \"<list>\" not in lines[line_index-1]: #Check if table or list is the previous item on the page, then they wont be a header\n",
    "                    header=lines[line_index-1]\n",
    "                else:\n",
    "                    header=\"\"           \n",
    "                list_pattern = re.compile(r'<list>(.*?)</list>', re.DOTALL)   ## Grab all list contents within the list xml tags        \n",
    "                list_match = re.search(list_pattern, line)\n",
    "                list_ = list_match.group(1)\n",
    "                list_lines=list_.split(\"\\n\")                \n",
    "\n",
    "                curr_chunk = []  \n",
    "                words=len(re.findall(r'\\w+', str(current_chunk)))  #start word count from any existing chunk\n",
    "                # Iterate through the items in the list\n",
    "                for item in list_lines:\n",
    "                    curr_chunk.append(item)         \n",
    "                    words+=len(re.findall(r'\\w+', item)) \n",
    "\n",
    "                    if words >= max_words: #  \n",
    "                        words = 0 # restart word count to zero\n",
    "                        \n",
    "                        list_chunk=\"\\n\".join(curr_chunk)\n",
    "                        if header: # If header  attach header to table                         \n",
    "                            if current_chunk and current_chunk[-1]==header: #check if header is in the chunk and remove to avoid duplicacy of header in chunk                        \n",
    "                                current_chunk.pop(-1)  \n",
    "                            header=header+\"\\n\" if not header.strip().endswith('\\n') else header                            \n",
    "                            if TITLE:\n",
    "                                current_chunk.insert(0, TITLE.strip())                          \n",
    "                            current_chunk.extend([header]+[list_chunk]) \n",
    "                            page_chunks.append(current_chunk)                            \n",
    "                            entire_list_chunk.append([header]+[list_chunk])\n",
    "                        else:\n",
    "                            if TITLE:\n",
    "                                current_chunk.insert(0, TITLE.strip())\n",
    "                            current_chunk.extend([list_chunk])\n",
    "                            page_chunks.append(current_chunk)\n",
    "                            entire_list_chunk.append([list_chunk])\n",
    "                       \n",
    "                        num_words=0\n",
    "                        current_chunk=[]\n",
    "                        curr_chunk = []\n",
    "\n",
    "                if curr_chunk  and lines.index(line) == len(lines)-1: #if list chunk still remaining and list is last item in page append as last chunk\n",
    "                    list_chunk=\"\\n\".join(curr_chunk)\n",
    "                    if header: \n",
    "                        if current_chunk and current_chunk[-1]==header: #check if header is in the chunk and remove to avoid duplicacy of header in chunk\n",
    "                            current_chunk.pop(-1)\n",
    "                        header=header+\"\\n\" if not header.strip().endswith('\\n') else header\n",
    "                        if TITLE:\n",
    "                            current_chunk.insert(0, TITLE.strip())\n",
    "                        current_chunk.extend([header]+[list_chunk])\n",
    "                        page_chunks.append(current_chunk)\n",
    "                        entire_list_chunk.append([header]+[list_chunk])\n",
    "                    else:\n",
    "                        if TITLE:\n",
    "                            current_chunk.insert(0, TITLE.strip())\n",
    "                        current_chunk.extend([list_chunk])\n",
    "                        page_chunks.append(current_chunk)\n",
    "                        entire_list_chunk.append([list_chunk])\n",
    "                    num_words=0\n",
    "                    current_chunk=[]\n",
    "                elif curr_chunk and lines.index(line) != len(lines)-1: #if list is not last item in page and max word threshold is not reached, send to next loop          \n",
    "                    list_chunk=\"\\n\".join(curr_chunk)\n",
    "                    if header:               \n",
    "                        if current_chunk and current_chunk[-1]==header: #check if header is in the chunk and remove to avoid duplicacy of header in chunk\n",
    "                            current_chunk.pop(-1)\n",
    "                        header=header+\"\\n\" if not header.strip().endswith('\\n') else header                         \n",
    "                        current_chunk.extend([header]+[list_chunk])\n",
    "                    else:\n",
    "                        current_chunk.extend([list_chunk])                  \n",
    "                    num_words=words\n",
    "            \n",
    "            \n",
    "    if current_chunk:\n",
    "        if TITLE:\n",
    "            current_chunk.insert(0, TITLE.strip())\n",
    "        page_chunks.append(current_chunk)\n",
    "        entire_list_chunk.append(current_chunk)\n",
    "        current_chunk=[]\n",
    "    chunks[page] = page_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "id": "9501b756-ba00-48f7-a400-49bae1d834d6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1:\n",
      "NOTE 5 - ACQUISITIONS, GOODWILL, AND ACQUIRED INTANGIBLE ASSETS\n",
      "(1) Primarily includes changes in foreign exchange rates.\n",
      "Intangible Assets\n",
      "Acquired identifiable intangible assets are valued primarily by using discounted cash flows. These assets are included within \"Other assets\" on our consolidated balance sheets and consist of the following (in millions):\n",
      "\n",
      "|December 31, ||||||\n",
      "0||2021 |||2022 |||\n",
      "1||Acquired Intangibles, Gross (1) |Accumulated Amortization (1) |Acquired Intangibles, Net |Acquired Intangibles, Gross (1) |Accumulated Amortization (1) |Acquired Intangibles, Net |Weighted Average Life Remaining \n",
      "2|Finite-lived intangible assets (2): |||||||\n",
      "3|Marketing-related |$ 2,286 |$ (548) |$ 1,738 |$ 2,407 |$ (601) |$ 1,806 |18.6 \n",
      "4|Contract-based |2,327 |(565) |1,762 |3,661 |(813) |2,848 |12.8 \n",
      "5|Technology- and content-based |976 |(610) |366 |883 |(643) |240 |3.2 \n",
      "6|Customer-related |197 |(103) |94 |184 |(128) |56 |2.2 \n",
      "\n",
      "\n",
      "Chunk 2:\n",
      "Acquired identifiable intangible assets are valued primarily by using discounted cash flows. These assets are included within \"Other assets\" on our consolidated balance sheets and consist of the following (in millions):\n",
      "\n",
      "\n",
      "|December 31, ||||||\n",
      "7|Total finite-lived intangible assets |$ 5,786 |$ (1,826) |$ 3,960 |$ 7,135 |$ (2,185) |$ 4,950 |14.4 \n",
      "8|IPR&D and other (3) |$ 1,147 ||$ 1,147 |$ 1,147 ||$ 1,147 |\n",
      "9|Total acquired intangibles |$ 6,933 |$ (1,826) |$ 5,107 |$ 8,282 |$ (2,185) |$ 6,097 |\n",
      "(1) Excludes the original cost and accumulated amortization of fully-amortized intangibles. \n",
      "(2) Finite-lived intangible assets, excluding acquired video content, have estimated useful lives of between one and twenty-five years, and are being amortized to operating expenses on a straight-line basis. \n",
      "(3) Intangible assets acquired in a business combination that are in-process and used in research and development activities are considered indefinite-lived until the completion or abandonment of the research and development efforts. Once the research and development efforts are completed, we determine the useful life and begin amortizing the assets.\n",
      "\n",
      "\n",
      "Chunk 3:\n",
      "Amortization expense for acquired finite-lived intangibles was $509 million, $512 million, and $604 million in 2020, 2021, and 2022. Expected future amortization expense of acquired finite-lived intangible assets as of December 31, 2022 is as follows (in millions):\n",
      "Year Ended December 31,\n",
      "\n",
      "2023 |$ 530 \n",
      "0|2024 |456 \n",
      "1|2025 |371 \n",
      "2|2026 |324 \n",
      "3|2027 |314 \n",
      "4|Thereafter |2,955 \n",
      "5||$ 4,950 \n",
      "55\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, chunk in enumerate(chunks[84], start=1):\n",
    "    print(f'Chunk {i}:')\n",
    "    for item in chunk:\n",
    "        print(item)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f459d61-102c-44c4-8b65-245b74c5d883",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pip install requests-aws4auth\n",
    "# pip install opensearch-py\n",
    "from opensearchpy import OpenSearch, RequestsHttpConnection\n",
    "from requests_aws4auth import AWS4Auth\n",
    "\n",
    "\"\"\"\n",
    "This is Amazon opensearch domain that uses IAM for authetication\n",
    "\"\"\"\n",
    "domain_endpoint = \"AMAZON OPENSEARCH PROVISIONED DOMAIN ENDPOINT\"\n",
    "service = 'es'\n",
    "credentials = boto3.Session().get_credentials()\n",
    "awsauth = AWS4Auth(credentials.access_key, credentials.secret_key, \"us-east-1\", service, session_token=credentials.token)\n",
    "os_ = OpenSearch(\n",
    "    hosts = [{'host': domain_endpoint, 'port': 443}],\n",
    "    http_auth = awsauth,\n",
    "    use_ssl = True,\n",
    "    verify_certs = True,\n",
    "    timeout=120,        \n",
    "    # http_compress = True, # enables gzip compression for request bodies\n",
    "    connection_class = RequestsHttpConnection\n",
    ")\n",
    "\n",
    "mapping = {\n",
    "  'settings': {\n",
    "    'index': {  \n",
    "      'knn': True,\n",
    "      \"knn.algo_param.ef_search\": 64,            \n",
    "    }\n",
    "      },\n",
    "\n",
    "      'mappings': {  \n",
    "        'properties': {\n",
    "          'embedding': {\n",
    "            'type': 'knn_vector', \n",
    "            'dimension': 384, #change as per sequence length of Embedding Model\n",
    "            \"method\": {\n",
    "              \"name\": \"hnsw\",       \n",
    "              \"space_type\": \"l2\",\n",
    "              \"engine\": \"lucene\",\n",
    "              \"parameters\": {\n",
    "                 \"ef_construction\": 72,\n",
    "                 \"m\":  72\n",
    "               }\n",
    "            }\n",
    "          },\n",
    "\n",
    "          'passage_id': {\n",
    "            'type': 'keyword'\n",
    "          },\n",
    "\n",
    "          'passage': {\n",
    "            'type': 'text'\n",
    "          },\n",
    "\n",
    "          'doc_id': {\n",
    "            'type': 'keyword'\n",
    "          },\n",
    "        \n",
    "          'table': {\n",
    "            'type': 'text'\n",
    "          },\n",
    "\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "# st.write(mapping)\n",
    "domain_index = f\"experiment9\"    \n",
    "\n",
    "if not os_.indices.exists(index=domain_index):        \n",
    "    os_.indices.create(index=domain_index, body=mapping)\n",
    "    # Verify that the index has been created\n",
    "    if os_.indices.exists(index=domain_index):\n",
    "        print(f\"Index {domain_index} created successfully.\")\n",
    "    else:\n",
    "        print(f\"Failed to create index '{domain_index}'.\")\n",
    "else:\n",
    "    print(f'{domain_index} Index already exists!')\n",
    "\n",
    "i = 1\n",
    "import boto3\n",
    "SAGEMAKER=boto3.client('sagemaker-runtime')\n",
    "for page, chunkks in chunks.items(): # Iterate through dict with chunk page# and content\n",
    "    chunk_id = page # take care of multiple chunks in same page (*) is used as delimiter\n",
    "   \n",
    "    \n",
    "    for chunk in chunkks:\n",
    "        passage_chunk=\"\\n\".join(chunk)\n",
    "        payload = {'text_inputs': [passage_chunk]}\n",
    "        payload = json.dumps(payload).encode('utf-8')\n",
    "\n",
    "        response = SAGEMAKER.invoke_endpoint(EndpointName=\"jumpstart-dft-hf-textembedding-all-minilm-l6-v2-2x\", \n",
    "                                                    ContentType='application/json',  \n",
    "                                                    Body=payload)\n",
    "\n",
    "        model_predictions = json.loads(response['Body'].read())\n",
    "        embedding = model_predictions['embedding'][0]\n",
    "        table=[]\n",
    "        if page in table_page:\n",
    "            for ids,item in enumerate(table_page[page]):\n",
    "                header=table_header_dict[page][ids]\n",
    "                if header.strip():\n",
    "                    header=f\"<table_header>{header}</table_header>\"\n",
    "                tsv=f\"<table>{item}</table>\"\n",
    "                table.append(header)\n",
    "                table.append(tsv)\n",
    "            table=\"\\n\".join(table)                \n",
    "        documentt = { \n",
    "            'doc_id':doc_id, #doc name\n",
    "            'passage_id': chunk_id, #page number\n",
    "            'passage': passage_chunk, \n",
    "            'embedding': embedding,\n",
    "            'table':table #table\n",
    "        }\n",
    "        try:\n",
    "            response = os_.index(index=domain_index, body=documentt)\n",
    "            i += 1\n",
    "            # Check the response to see if the indexing was successful\n",
    "            if response[\"result\"] == \"created\":\n",
    "                print(f\"Document indexed successfully with ID: {response['_id']}\")\n",
    "            else:\n",
    "                print(\"Failed to index document.\")\n",
    "        except RequestError as e:\n",
    "            logging.error(f\"Error indexing document to index '{domain_index}': {e}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3f7c64-1ddb-4c4a-a050-67669632790b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "question=\"operating income loss in 2022 for North america\"\n",
    "response= SAGEMAKER.invoke_endpoint(EndpointName=\"ALL-MINI-LM2 SageMaker Model Endpoint\", \n",
    "                                                    ContentType='application/json',  \n",
    "                                                    Body=json.dumps({'text_inputs': [question]}))\n",
    "model_predictions = json.loads(response['Body'].read())\n",
    "embedding = model_predictions['embedding'][0]\n",
    "\n",
    "query = {\n",
    "    'size': 3,\n",
    "    'query': {\n",
    "        \"knn\": {\n",
    "          \"embedding\": {\n",
    "            \"vector\": embedding,\n",
    "            \"k\": 3\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "response = os_.search(index=domain_index, body=query)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "id": "6c3a127e-74c4-4500-a48d-084d026108a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "res=response['hits']['hits']\n",
    "score = [str(x['_score']) for x in res]  #retrieval score\n",
    "passage = [x['_source']['passage'] for x in res] #retrieved passages\n",
    "page_no = [x['_source']['passage_id'] for x in res] #doc page number of chunks\n",
    "doc_name = [x['_source']['doc_id'] for x in res] # doc names\n",
    "tables=[x['_source']['table'] for x in res] # tables in the corresponding chunk doc pages\n",
    "\n",
    "## Concatenate passages and tables\n",
    "passages=\"\"\n",
    "tab=\"\"\n",
    "for  ids,text in enumerate(passage):\n",
    "    passages+=f\"<{p.ordinal(ids+1)}_passage>\\n{text}\\n</{p.ordinal(ids+1)}_passage>\\n\"\n",
    "    tab+=f\"<{p.ordinal(ids+1)}_passage_table>\\n{tables[ids]}\\n</{p.ordinal(ids+1)}_passage_table>\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1bbeba-2fd9-4775-aec2-8a8c287e8655",
   "metadata": {},
   "source": [
    "## Bedrock Anthropic LLM Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "id": "edc0c8ab-5cf5-4200-829f-08c576db2d45",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt_template=f\"\"\"You are a helpful, obedient and truthful financial assistance.\n",
    "\n",
    "<document>\n",
    "{passages}\n",
    "</document>          \n",
    "\n",
    "<additional_information>\n",
    "{tab}\n",
    "</additional_information>\n",
    "\n",
    "<instructions>\n",
    "When providing your response based on the document:\n",
    "1. Understand the question to know what is being asked of you.\n",
    "2. Review the entire document provided and check if it contains relevant information to answer the question. Only pay attention to passages with relevant information.\n",
    "3. If the document is sufficient to answer the question, provide a comprehensive answer ENTIRELY based on the document provided. DO NOT make up answers not present in the document.\n",
    "4. If the answer is not available in the document, say so.\n",
    "</instructions>\n",
    "\n",
    "Question: {question}\"\"\"\n",
    "prompt=f\"\\n\\nHuman:{prompt_template}\\n\\nAssistant: Based on the document,\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063e1c51-508c-4f5b-8928-b98a2a10e2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model='anthropic.claude-v2'\n",
    "prompts={\n",
    "  \"prompt\": prompt,\n",
    "  \"max_tokens_to_sample\": 300,\n",
    "  \"temperature\": 0.1,\n",
    "  # \"top_k\": 250,\n",
    "  # \"top_p\": 1,  \n",
    "   \n",
    "}\n",
    "prompts=json.dumps(prompts)\n",
    "response = bedrock_runtime.invoke_model_with_response_stream(body=prompts, modelId=model, accept=\"application/json\",  contentType=\"application/json\")\n",
    "output = bedrock_streemer(response,model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16fe1cf-998b-4469-bf08-e9b77feead7b",
   "metadata": {},
   "source": [
    "## Mixtral 8x7b Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "id": "81104193-61c8-481e-ae83-0dd36fa46b87",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt=f\"\"\"<s><<SYS>>[INST]\n",
    "You are a helpful, obedient and truthful assistant. You will only provide answers entirely based on the document provided below.\n",
    "\n",
    "Here is a document:\n",
    "####\n",
    "{passages}\n",
    "####\n",
    "\n",
    "Here is additional information:\n",
    "####\n",
    "{tab}\n",
    "####\n",
    "\n",
    "When providing your response based on the provided document:\n",
    "1. Understand the question to know what is being asked of you.\n",
    "2. Review the entire document provided and check if it contains relevant information to answer the question. Only pay attention to sections with relevant information.\n",
    "3. If the document is sufficient to answer the question, provide a comprehensive answer ENTIRELY based on the document provided. DO NOT make up answers not present in the document.\n",
    "4. If the answer is not available in the document, say so.<</SYS>>\n",
    "\n",
    "Question: {question}[/INST]\n",
    "Answer: According to the document provided,\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e476170b-492b-4b41-8e1d-afdab9b52c7c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "payload = {\n",
    "       \"inputs\": prompt,\n",
    "        \"parameters\": {\"max_new_tokens\": 300, \n",
    "                       # \"top_p\": params['top_p'] if params['top_p']<1 else 0.99 ,\n",
    "                       # \"temperature\": 0.1,\n",
    "                       \"return_full_text\": False,},\n",
    "    \"stream\": True\n",
    "    } \n",
    "output=SAGEMAKER.invoke_endpoint_with_response_stream(Body=json.dumps(payload), EndpointName=\"MIXTRAL ENDPOINT SAGEAMKER\",ContentType=\"application/json\")\n",
    "answer=jumpstart_streemer(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13f8083-f382-409c-af8b-dba24a74d3da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.c5.large",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
